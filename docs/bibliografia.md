# Bibliografia Consolidata per RLCF: Letteratura Autorevole per Supportare le Tesi del Framework

## Indice

1. [Expert Knowledge Aggregation e Computational Peer Review](#1-expert-knowledge-aggregation-e-computational-peer-review)
2. [Constitutional AI e Democratic Governance](#2-constitutional-ai-e-democratic-governance)
3. [Uncertainty Quantification e Epistemic Diversity](#3-uncertainty-quantification-e-epistemic-diversity)
4. [Legal AI e Domini Normativi Complessi](#4-legal-ai-e-domini-normativi-complessi)
5. [Peer Assessment Enhancement con AI](#5-peer-assessment-enhancement-con-ai)
6. [Implicazioni per il Paper RLCF](#6-implicazioni-per-il-paper-rlcf)

---

## 1. Expert Knowledge Aggregation e Computational Peer Review

### 1.1 Aggregating and Updating Experts' Knowledge: An Experimental Evaluation of Five Classification Techniques

**Citazione completa:**
Barrett, A.R., Hornik, K., Medsker, L., Subramanian, V., Tavana, M., Taylor, W.A., and Yoon, Y. "Aggregating and Updating Experts' Knowledge: An Experimental Evaluation of Five Classification Techniques." *ScienceDirect*, February 10, 1999.

**Rilevanza per RLCF:**
Questo paper pionieristico confronta cinque tecniche per aggregare expertise da fonti multiple, includendo metodi statistici classici (logit regression, discriminant analysis), pattern classification (ID3), k-NN, e neural networks. Il neural network method ha dimostrato superiore robustezza e accuratezza predittiva.

**Come supporta RLCF:**

- Fornisce precedenti empirici per l'aggregazione computazionale di expertise multiple
- Dimostra che metodi di aggregazione sofisticati superano approcci semplici
- Supporta il nostro approccio di Dynamic Authority Scoring che usa pesi differenziati

**Estratto chiave:**
*"When knowledge is elicited from multiple experts, it is necessary to combine the multiple sources of expertise in order to arrive at a single knowledge base... The neural net method was shown to outperform the other methods in robustness and predictive accuracy."*

---

### 1.2 Expertise, Social Influence, and Knowledge Aggregation in Distributed Information Processing

**Citazione completa:**
Multiple authors. "Expertise, Social Influence, and Knowledge Aggregation in Distributed Information Processing." *Artificial Life*, MIT Press, Volume 29, Issue 1, pages 37-XX, January 2, 2023.

**Rilevanza per RLCF:**
Introduce la Regulatory Theory of Social Influence (RTSI) per knowledge aggregation in sistemi distribuiti. Implementa un algoritmo basato su RTSI per aggregazione di conoscenza in contesti di social choice.

**Come supporta RLCF:**

- Fornisce teoria psicologica per il nostro approccio di community feedback
- Dimostra l'importanza di considerare influenza sociale nell'aggregazione
- Supporta il nostro Devil's Advocate System come meccanismo anti-groupthink

**Estratto chiave:**
*"Social influence consists of not only sources trying to influence targets, but also targets seeking sources by whom to be influenced and learning what processing rules those sources are using... We say 'select a process,' because many methods for efficient, effective, and non-manipulable knowledge aggregation have been studied."*

---

### 1.3 AAAI Launches AI-Powered Peer Review Assessment System

**Citazione completa:**
Smith, Stephen (AAAI President). "AAAI Launches AI-Powered Peer Review Assessment System." *Association for the Advancement of Artificial Intelligence*, June 4, 2025.

**Rilevanza per RLCF:**
AAAI ha lanciato un pilot program usando LLMs per supportare (non sostituire) peer review umano. Include supplementary first-stage reviews e discussion summary assistance.

**Come supporta RLCF:**

- Valida l'approccio di AI come complemento all'expertise umana
- Dimostra implementazione pratica di hybrid human-AI review systems
- Enfatizza "primacy of human expertise and judgment"

**Estratti chiave:**

- *"This pilot represents a careful, measured approach to incorporating new technology into the scientific review process. We're exploring how LLMs can complementâ€”not replaceâ€”the irreplaceable expertise and judgment of our human reviewers."*
- *"No Displacement of Human Reviewers: No human reviewers are being replaced at any stage of the process."*

---

### 1.4 Computational Support for Academic Peer Review: A Perspective from Artificial Intelligence

**Citazione completa:**
Price, S. and Flach, P.A. "Computational Support for Academic Peer Review: A Perspective from Artificial Intelligence." *ResearchGate*, February 21, 2017.

**Rilevanza per RLCF:**
Descrive soluzioni early-stage per automatizzare fasi chiave del peer review process. Include reviewer assignment algorithms e sentiment analysis di review proposals.

**Come supporta RLCF:**

- Fornisce precedenti per computational peer review
- Identifica sfide specifiche nell'automazione del peer review
- Supporta necessitÃ  di expert matching (relativo al nostro Dynamic Authority)

**Estratto chiave:**
*"Price & Flach (2017) exploited computational support and described some early solutions for automating key stages in the established academic peer review process... They found that review sentiments positively correlate with review scores, and manually coded review sentiments can accurately predict whether proposals are funded."*

---

## 2. Constitutional AI e Democratic Governance

### 2.1 Public Constitutional AI

**Citazione completa:**
Abiri, Gilad. "Public Constitutional AI." *Georgia Law Review*, Volume 59, No. 2, Art. 5, pages 601-669, June 24, 2024.

**Rilevanza per RLCF:**
Propone un framework per coinvolgere il pubblico nella creazione di costituzioni AI che devono essere usate nel training di frontier AI models. Affronta deficit di opacitÃ  e comunitÃ  politica nell'AI governance.

**Come supporta RLCF:**

- Fornisce framework teorico per il nostro Pillar 3 (Constitutional Governance)
- Supporta l'importanza della trasparenza e accountability
- Valida l'approccio di coinvolgimento della comunitÃ  nell'AI alignment

**Estratti chiave:**

- *"Public Constitutional AI mitigates the opacity deficit. It does so by rendering the principles and values governing AI systems more transparent and accessible to the forms of public discourse and contestation essential to democratic legitimacy."*
- *"By grounding the development of AI principles in the social context and shared experiences of a particular political community, Public Constitutional AI helps bridge the gap between the abstract logic of algorithms and the situated, contextual judgments that legitimize authority in a democracy."*

---

### 2.2 Collective Constitutional AI: Aligning a Language Model with Public Input

**Citazione completa:**
Anthropic and Collective Intelligence Project. "Collective Constitutional AI: Aligning a Language Model with Public Input." *Anthropic Blog*, 2024.

**Rilevanza per RLCF:**
Documenta esperimento pratico con ~1000 americani per creare democraticamente una costituzione AI usando la piattaforma Polis per deliberazione online.

**Come supporta RLCF:**

- Dimostra fattibilitÃ  pratica di democratic AI governance
- Fornisce lessons learned per implementazione di community feedback
- Mostra sfide tecniche di Constitutional AI training

**Estratti chiave:**

- *"We believe that our work may be one of the first instances in which members of the public have collectively directed the behavior of a language model via an online deliberation process."*
- *"Constitutional AI training is hard. We are not sure we would have been able to train our own models using Constitutional AI (CAI) without working directly and very closely with the original developers. CAI training is more complicated than we thought."*

---

### 2.3 Constitutional AI: Harmlessness from AI Feedback

**Citazione completa:**
Bai, Yuntao, Kadavath, Saurav, Kundu, Sandipan, et al. "Constitutional AI: Harmlessness from AI Feedback." *arXiv preprint arXiv:2212.08073*, December 2022.

**Rilevanza per RLCF:**
Paper fondamentale che introduce Constitutional AI, metodo per allineare LLMs a principi normativi high-level scritti in una costituzione.

**Come supporta RLCF:**

- Fornisce base metodologica per constitutional approaches all'alignment
- Dimostra scalabilitÃ  di self-supervision con principi espliciti
- Supporta trasparenza attraverso principi human-understandable

**Estratto chiave:**
*"Constitutional AI represents a step towards this goal, offering a model for how AI might be brought under democratic control and made answerable to the common good. Just as constitutions limit and guide the exercise of governmental power, Constitutional AI seeks to hardcode explicit principles and values into AI models."*

---

## 3. Uncertainty Quantification e Epistemic Diversity

### 3.1 From Aleatoric to Epistemic: Exploring Uncertainty Quantification Techniques in Artificial Intelligence

**Citazione completa:**
Wang, Tianyang, Wang, Yunze, Zhou, Jun, et al. (19 autori totali). "From Aleatoric to Epistemic: Exploring Uncertainty Quantification Techniques in Artificial Intelligence." *arXiv preprint arXiv:2501.03282*, January 5, 2025.

**Rilevanza per RLCF:**
Review completa di tecniche UQ in AI, distinguendo tra incertezze aleatoriche (data noise) ed epistemiche (model limitations). Copre metodi probabilistici, ensemble learning, e approcci sampling-based.

**Come supporta RLCF:**

- Fornisce fondamenti matematici per il nostro Pillar 2 (Uncertainty Preservation)
- Supporta uso di entropia per quantificare disagreement
- Valida importanza di distinguere tipi di incertezza

**Estratti chiave:**

- *"Aleatoric uncertainty arises from the intrinsic randomness and noise within the data... This type of uncertainty is typically irreducible. In contrast, epistemic uncertainty stems from the model's limitations in understanding the data distribution."*
- *"Ensemble methods leverage the diversity among multiple models to estimate uncertainty... predictions are aggregated to compute the mean and variance."*

---

### 3.2 Aleatoric and Epistemic Uncertainty in Machine Learning: An Introduction to Concepts and Methods

**Citazione completa:**
HÃ¼llermeier, Eyke and Waegeman, Willem. "Aleatoric and Epistemic Uncertainty in Machine Learning: An Introduction to Concepts and Methods." *Machine Learning*, Volume 110, pages 457-506, March 8, 2021.

**Rilevanza per RLCF:**
Fornisce introduzione completa a uncertainty in ML, distinguendo tra tipi e presentando metodi per quantificazione.

**Come supporta RLCF:**

- Base teorica per uncertainty quantification nel nostro framework
- Supporta necessitÃ  di preservare incertezza epistemica
- Valida approccio di non forzare consenso artificiale

**Estratto chiave:**
*"The notion of uncertainty is of major importance in machine learning... new problems and challenges have recently been identified by machine learning scholars... this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic."*

---

### 3.3 Beyond Quantification: Navigating Uncertainty in Professional AI Systems

**Citazione completa:**
Author not specified. "Beyond Quantification: Navigating Uncertainty in Professional AI Systems." *arXiv preprint arXiv:2509.03271*, September 3, 2025.

**Rilevanza per RLCF:**
Propone "participatory refinement" per sviluppare framework di espressione dell'incertezza in domini professionali. Critica approcci puramente quantitativi.

**Come supporta RLCF:**

- Supporta nostro approccio di preservazione incertezza qualitativa
- Valida importanza di coinvolgimento community in definizione uncertainty
- Fornisce framework per "epistemic and normative category formation"

**Estratti chiave:**

- *"Unlike standard active learning approaches that use uncertainty quantification to improve predictive accuracy, participatory refinement focuses on collectively developing new representational frameworks for uncertainty expression."*
- *"When professional communities collectively refine uncertainty expression frameworks, they simultaneously negotiate the boundaries of what counts as legitimate uncertainty."*

---

## 4. Legal AI e Domini Normativi Complessi

### 4.1 Judicial Requirements for Generative AI in Legal Reasoning

**Citazione completa:**
Multiple authors. "Judicial Requirements for Generative AI in Legal Reasoning." *arXiv preprint arXiv:2508.18880*, August 26, 2025.

**Rilevanza per RLCF:**
Analizza requisiti specifici per AI nel legal reasoning usando framework IRAC (Issue-Rule-Application-Conclusion). Identifica sfide in interpretazione e applicazione di regole legali.

**Come supporta RLCF:**

- Dimostra inadeguatezza di approcci generici per domini legali
- Supporta necessitÃ  di expert validation in domini normativi
- Valida importanza di preservare ambiguitÃ  e interpretazioni multiple

**Estratti chiave:**

- *"AI may struggle to interpret the connecting factors that determine jurisdiction, such as a defendant's habitual residence or where the damage occurred which normally allow for judicial discretion and context-sensitive application."*
- *"The findings indicate that while these techniques can address specific challenges, significant challenges remain, particularly in tasks requiring discretion and transparent, justifiable reasoning."*

---

### 4.2 Explainable AI and Law: An Evidential Survey

**Citazione completa:**
Multiple authors. "Explainable AI and Law: An Evidential Survey." *Digital Society*, Volume 3, Article 81, December 19, 2023.

**Rilevanza per RLCF:**
Survey di explainable AI nel dominio legale, evidenziando necessitÃ  di trasparenza e accountability in legal AI systems.

**Come supporta RLCF:**

- Supporta nostro focus su trasparenza (Pillar 3)
- Dimostra che legal domain richiede "high level of accountability"
- Valida necessitÃ  di explainability per legal decision-making

**Estratti chiave:**

- *"Decisions made by legal adjudicators and administrative decision-makers often found upon a reservoir of stored experiences, from which is drawn a tacit body of expert knowledge."*
- *"This raises particular issues within the legal domain, which requires a high level of accountability, thus transparency."*

---

### 4.3 LEXam: Benchmarking Legal Reasoning on 340 Law Exams

**Citazione completa:**
Multiple authors. "LEXam: Benchmarking Legal Reasoning on 340 Law Exams." *arXiv preprint arXiv:2505.12864*, May 19, 2025.

**Rilevanza per RLCF:**
Benchmark multilingual per legal reasoning con 2,841 open-ended e 2,045 MCQ questions. Mostra che SOTA models faticano con multi-step legal reasoning.

**Come supporta RLCF:**

- Evidenza empirica che modelli attuali sono inadeguati per legal reasoning
- Supporta necessitÃ  di approcci specializzati per domini normativi
- Dimostra importanza di preservare complessitÃ  invece di semplificare

**Estratti chiave:**

- *"Experimental results show that state-of-the-art (SOTA) models struggle with multi-step reasoning and applying legal rules and principles to novel scenarios."*
- *"Legal reasoning is a critical frontier for large language models... requiring specialized domain knowledge and advanced reasoning abilities such as precedent interpretation, statutory analysis, and legal inference."*

---

## 5. Peer Assessment Enhancement con AI

### 5.1 Enhancing Peer Assessment with Artificial Intelligence

**Citazione completa:**
Multiple authors. "Enhancing Peer Assessment with Artificial Intelligence." *International Journal of Educational Technology in Higher Education*, Volume 22, Article 501, January 21, 2025.

**Rilevanza per RLCF:**
Survey di 79 papers su AI-enhanced peer assessment. Trova che AI migliora peer assessment ma non puÃ² sostituire human judgment.

**Come supporta RLCF:**

- Valida approccio hybrid human-AI per assessment
- Supporta uso di AI per aggregare feedback diversi
- Dimostra importanza di preservare "imprecision of words"

**Estratti chiave:**

- *"The vast majority of the 79 papers in the review found that artificial intelligence improved peer assessment."*
- *"Four papers were concerned with the imprecision of words... Linguistic terms provided by assessors to evaluate criteria... were converted into interval type-2 fuzzy sets. The footprint of uncertainty represented variability in meanings of the linguistic terms as assessed by different experts."*

---

### 5.2 Artificial Intelligence to Support Publishing and Peer Review: A Summary and Review

**Citazione completa:**
Kousha, K. and Thelwall, M. "Artificial Intelligence to Support Publishing and Peer Review: A Summary and Review." *Learned Publishing*, Wiley Online Library, August 8, 2023.

**Rilevanza per RLCF:**
Review completa di AI tools per supportare peer review, includendo reviewer recommendation systems e automated quality checks.

**Come supporta RLCF:**

- Fornisce landscape di AI tools esistenti per peer review
- Supporta fattibilitÃ  tecnica del nostro approccio
- Identifica gap che RLCF puÃ² colmare

**Estratti chiave:**

- *"Clarivate's Reviewer Locator automatically suggests reviewers based on data from the Web of Science and Publons peer review databases."*
- *"Technology is being developed to support the peer review processes of journals, conferences, funders, universities, and national research evaluations."*

---

## 6. Implicazioni per il Paper RLCF

### 6.1 Validazione Teorica del Framework

La letteratura identificata fornisce solida validazione teorica per RLCF:

1. **Precedenti per Knowledge Aggregation**: Papers su expert aggregation dimostrano fattibilitÃ  e superioritÃ  di approcci computazionali sofisticati
2. **Fondamenti per Constitutional Governance**: Lavori su Public/Collective Constitutional AI forniscono framework teorici e pratici
3. **Base Matematica per Uncertainty**: Letteratura UQ fornisce rigorosi fondamenti per preservazione incertezza
4. **Evidenza di NecessitÃ  Domain-Specific**: Legal AI papers dimostrano chiaramente inadeguatezza di approcci generici

### 6.2 Supporto per i Quattro Pilastri

**Pillar 1 - Dynamic Authority Scoring:**

- Supportato da letteratura su expert aggregation e reviewer recommendation
- Validato da necessitÃ  di expertise dinamica in peer review

**Pillar 2 - Uncertainty-Preserving Aggregation:**

- Fondamenti matematici da UQ literature
- Supporto per uso entropia di Shannon
- Validazione di preservare disagreement come informazione

**Pillar 3 - Constitutional Governance:**

- Framework teorici completi da Public Constitutional AI
- Esperimenti pratici da Collective Constitutional AI
- Supporto per trasparenza e democratic governance

**Pillar 4 - Devil's Advocate System:**

- Supportato da RTSI theory su social influence
- Validato da necessitÃ  di prevenire groupthink
- Precedenti in peer review per challenging consensus

### 6.3 Posizionamento nel Landscape Accademico

RLCF emerge come:

- **Evoluzione Naturale**: Non proposta isolata ma convergenza di multiple linee di ricerca
- **Risposta a Gap Identificati**: Affronta specifiche limitazioni documentate di RLHF
- **Framework Integrativo**: Combina insights da AI, law, social science, democratic theory
- **Contributo Pratico**: Offre implementazione concreta, non solo teoria

### 6.4 Raccomandazioni per Rafforzamento del Paper

1. **Citare Precedenti Teorici**: Integrare riferimenti a Public Constitutional AI e RTSI
2. **Evidenziare Evidenza Empirica**: Usare risultati da LEXam e peer assessment studies
3. **Confrontare con Approcci Esistenti**: Posizionare rispetto a Collective Constitutional AI
4. **Enfatizzare NovitÃ **: Chiarire contributi unici (es. entropia per disagreement, Devil's Advocate)
5. **Fornire Roadmap Implementativa**: Usare lessons da Constitutional AI training challenges

### 6.5 Conclusione

Questa bibliografia consolidata dimostra che RLCF:

- Affronta problemi reali e documentati
- Si basa su solidi fondamenti teorici
- Integra insights da multiple discipline
- Offre soluzioni pratiche e implementabili
- Rappresenta evoluzione necessaria nell'AI alignment

Il framework RLCF emerge non come proposta speculativa, ma come risposta metodologicamente rigorosa e empiricamente motivata alle sfide dell'allineamento AI in domini normativi complessi.



# Pubblicazioni accademiche su RLHF per la Sezione 1.2: Fondamenti e Critiche

## Pubblicazioni seminali su RLHF

### 1. Deep Reinforcement Learning from Human Preferences (Christiano et al., 2017)

**Citazione completa:**
Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. "Deep Reinforcement Learning from Human Preferences." In *Advances in Neural Information Processing Systems 30 (NIPS 2017)*, pages 4299-4307, 2017.

**Rilevanza:** Questo è il paper fondamentale che ha introdotto la formulazione moderna di RLHF. Ha dimostrato che task complessi di RL possono essere risolti apprendendo da preferenze umane tra segmenti di traiettorie, piuttosto che richiedere funzioni di reward manualmente definite. L'innovazione chiave è stata l'uso di un reward model appreso da comparazioni umane per fornire feedback, richiedendo meno dell'1% delle interazioni dell'agente etichettate dagli umani. Questo lavoro, una collaborazione tra ricercatori OpenAI e DeepMind, ha stabilito la metodologia RLHF core utilizzata oggi.

---

### 2. Learning to Summarize from Human Feedback (Stiennon et al., 2020)

**Citazione completa:**
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. "Learning to Summarize from Human Feedback." In *Advances in Neural Information Processing Systems 33 (NeurIPS 2020)*, pages 3008-3021, 2020.

**Rilevanza:** Questo paper ha esteso RLHF ai modelli linguistici, specificamente per la summarization testuale. Ha dimostrato che addestrare modelli per ottimizzare le preferenze umane piuttosto che metriche tradizionali (come ROUGE) migliora significativamente la qualità dei riassunti. I modelli addestrati con RLHF hanno superato modelli molto più grandi addestrati solo con supervised learning. Questo lavoro è stato cruciale nello stabilire RLHF come tecnica efficace per allineare i modelli linguistici con le preferenze umane e ha aperto la strada a modelli come InstructGPT e ChatGPT.

---

### 3. Constitutional AI: Harmlessness from AI Feedback (Bai et al., 2022)

**Citazione completa:**
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. "Constitutional AI: Harmlessness from AI Feedback." *arXiv preprint arXiv:2212.08073*, December 2022.

**Rilevanza:** Constitutional AI rappresenta un'innovazione significativa in RLHF, addestrando sistemi AI a essere innocui attraverso "auto-miglioramento" con supervisione umana minima. Il metodo opera in due fasi: (1) fase di Supervised Learning dove l'AI genera auto-critiche e revisioni basate su una costituzione (lista di principi), poi fa fine-tuning sulle risposte riviste, e (2) fase RL usando "RL from AI Feedback" (RLAIF) dove label di preferenza generate dall'AI sostituiscono le label umane per l'addestramento all'innocuità. Questo paper è rilevante perché introduce un approccio scalabile all'allineamento AI ed è stato utilizzato per addestrare Claude di Anthropic.

---

## Critiche accademiche a RLHF

### Critiche su Annotator Bias e Task Simplification

#### 1. On the Algorithmic Bias of Aligning LLMs with RLHF: Preference Collapse and Matching Regularization

**Citazione completa:**
Jiancong Xiao, Ziniu Li, Xingyu Xie, Emily Getzen, Cong Fang, Qi Long, and Weijie J. Su. "On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization." *arXiv preprint arXiv:2405.16455*, 2024. (Accettato per pubblicazione in Journal of the American Statistical Association)

**Perché è rilevante:** Questo paper identifica un fenomeno chiamato "preference collapse" dove le preferenze minoritarie vengono virtualmente ignorate in RLHF standard, portando a estremi sbilanciamenti di preferenze (0% vs 100%). Questo rappresenta una forma estrema di semplificazione del task dove preferenze umane sfumate collassano in risultati binari.

**Come supporta il punto critico su Annotator Bias e Task Simplification:**

Il paper riporta che Ouyang et al. (2022) e Bai et al. (2022) hanno documentato tassi di disaccordo annotatore-annotatore e annotatore-ricercatore del 37% e 23% rispettivamente, anche per labeler umani addestrati. Nonostante questa variabilità significativa, RLHF standard produce modelli che **"esclusivamente preferiscono l'opinione maggioritaria e disregardano completamente qualsiasi opinione minoritaria"** anche quando il rapporto è 51%-49%.

**Estratto chiave:** *"In extreme cases, this bias could lead to a phenomenon we term preference collapse, where minority preferences are virtually disregarded... LLMs tend to disproportionately favor more frequent items over less frequent ones... opinion distributions generated by LLMs are often highly skewed towards the dominant viewpoints, often assigning over 99% probability to the dominant opinion."*

Questo dimostra che la struttura algoritmica di RLHF favorisce sistematicamente le preferenze maggioritarie e pattern più semplici, amplificando bias anche quando gli annotatori mostrano solo lievi preferenze. Gli annotatori non esperti tendono a convergere su risposte semplici e dirette, e RLHF amplifica questa tendenza eliminando risposte metodologicamente rigorose ma complesse che potrebbero dividere le opinioni.

---

#### 2. RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback

**Citazione completa:**
Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, and Tat-Seng Chua. "RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback." In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2024.

**Perché è rilevante:** Questo paper dimostra che **"risposte utili e coinvolgenti su contenuti ricchi di immagini sono tipicamente lunghe e complesse, rendendo solitamente non-ovvio decidere quale risposta sia preferibile... gli annotatori di solito affrontano dilemmi quando presentano risposte con rispettivi vantaggi e difetti."**

**Come supporta il punto critico su Annotator Bias e Task Simplification:**

Il paper identifica tre fattori nei dati di preferenza: (a) comportamento veramente preferito, (b) **"bias superficiale non-robusto correlato con i dati ma non correlato al giudizio umano,"** e (c) varianza linguistica. Il secondo fattore porta a problemi di "reward hacking" dove i modelli sfruttano pattern superficiali piuttosto che apprendere qualità genuina.

**Estratto chiave:** *"Common RLHF practices in LLMs collect human preference in the form of ranking labels... the practice faces several key challenges: (1) Annotation ambiguity. It can be non-obvious to annotate which response is superior using an overall ranking label due to the fine-grained nature of [preferred behavior], especially for complex responses... (2) Learning efficiency. The coarse-grained ranking feedback makes it difficult to accurately allocate credit to the desirable behaviors."*

Il paper dimostra esplicitamente che gli annotatori hanno difficoltà con risposte complesse e sfumate, e tendono a fare giudizi più chiari su output più semplici. Questa ambiguità nell'annotazione crea un bias sistematico verso risposte più facili da valutare, che tipicamente significa risposte più semplici e dirette piuttosto che metodologicamente rigorose o sfumate. Il paper mostra che usando feedback fine-grained (1.4k esempi) si possono superare prestazioni di RLHF tradizionale addestrato su 10x più dati (10k esempi), dimostrando l'inefficienza della raccolta di preferenze coarse-grained per task complessi.

---

#### 3. Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions

**Citazione completa:**
Mihir Parmar, Swaroop Mishra, Mor Geva, and Chitta Baral. "Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions." In *Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (EACL)*, pages 1771-1781, 2023.

**Perché è rilevante:** Gli annotatori **"captano pattern nelle istruzioni di crowdsourcing, che li predispongono a scrivere molti esempi simili che poi sono sovra-rappresentati nei dati raccolti."** Questo crea semplificazione sistematica poiché gli annotatori seguono i pattern più facili da replicare dalle istruzioni.

**Come supporta il punto critico su Annotator Bias e Task Simplification:**

**Estratto chiave:** *"In this work, we hypothesize that annotators pick up on patterns in the crowdsourcing instructions, which bias them to write many similar examples that are then over-represented in the collected data... Instruction examples often exhibit concrete patterns, which are propagated by crowdworkers to the collected data... instruction bias can lead to overestimation of model performance, and... models struggle to generalize beyond biases originating in the crowdsourcing instructions."*

Sebbene questo paper si concentri su benchmark NLU piuttosto che specificamente su RLHF, fornisce evidenza cruciale su come annotatori non esperti sistematicamente semplificano i task seguendo pattern dalle istruzioni. Questo si applica direttamente all'annotazione RLHF dove crowdworker generano o valutano risposte—tendono a replicare pattern semplici dalle linee guida piuttosto che impegnarsi con la piena complessità dei task, portando a dati di preferenza che favoriscono risposte dirette e pattern-matching rispetto a risposte sfumate e metodologicamente solide.

---

### Critiche su Collapse of Pluralism

#### 4. A Roadmap to Pluralistic Alignment (Sorensen et al., 2024)

**Citazione completa:**
Taylor Sorensen, Jared Moore, Jillian Fisher, Mitchell Gordon, Niloofar Mireshghallah, Christopher Michael Rytting, Andre Ye, Liwei Jiang, Ximing Lu, Nouha Dziri, Tim Althoff, and Yejin Choi. "A Roadmap to Pluralistic Alignment." In *Proceedings of the 41st International Conference on Machine Learning (ICML 2024)*, arXiv:2402.05070, 2024.

**Perché è rilevante:** Questo paper fornisce una critica fondamentale di come RLHF tratti la diversità umana come rumore invece che come segnale. Gli autori dimostrano che le procedure di allineamento standard riducono il pluralismo distribuzionale nei modelli rispetto ai baseline pre-allineamento.

**Come supporta il punto critico su Collapse of Pluralism:**

Il paper formalizza tre tipi di pluralismo che RLHF elimina:

- **Overton Pluralism:** Perdita dello spettro di risposte ragionevoli
- **Steerable Pluralism:** Incapacità di orientarsi per riflettere certe prospettive
- **Distributional Pluralism:** Fallimento nell'essere ben-calibrato alla distribuzione della popolazione

**Estratto chiave:** *"We use this framework to argue that current alignment techniques may be fundamentally limited for pluralistic AI; indeed, we highlight empirical evidence, both from our own experiments and from other work, that standard alignment procedures might reduce distributional pluralism in models."*

Il paper critica esplicitamente come RLHF presuma che i modelli debbano rappresentare un singolo "consenso" piuttosto che mantenere la ricca diversità di valori e prospettive umane. L'assunzione implicita è che i modelli debbano adattarsi alla preferenza umana "media", eliminando informazioni preziose sulla diversità. Gli esperimenti hanno dimostrato che modelli allineati con RLHF mostrano diversità di output significativamente ridotta rispetto al solo supervised fine-tuning (SFT), misurata attraverso multiple metriche di diversità.

---

#### 5. MaxMin-RLHF: Alignment with Diverse Human Preferences (Chakraborty et al., 2024)

**Citazione completa:**
Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Furong Huang, Dinesh Manocha, Amrit S. Bedi, and Mengdi Wang. "MaxMin-RLHF: Alignment with Diverse Human Preferences." In *Proceedings of the 41st International Conference on Machine Learning (ICML 2024)*, arXiv:2402.08925, 2024.

**Perché è rilevante:** Questo paper fornisce il **primo risultato formale di impossibilità** dimostrando che RLHF con singolo reward non può allinearsi con preferenze umane diverse. Questo è un risultato matematico rigoroso, non solo un'osservazione empirica.

**Come supporta il punto critico su Collapse of Pluralism:**

Il paper deriva un teorema di impossibilità formale (Teorema 3.3) dimostrando che quando le preferenze umane sono diverse, singoli reward model esibiscono lower bound di reward mismatch che scalano con la diversità tra sottopopolazioni. Il gap di allineamento aumenta proporzionalmente alla diversità di preferenze.

**Estratti chiave:**

- *"Single reward RLHF pipeline is insufficient to align with diverse human preferences... The assumption of a single ground truth reward is restrictive and can potentially subdue the preferences or opinions of minority groups, leading to societal biases."*
- **Evidenza empirica:** Esperimenti su GPT-2 hanno mostrato che quando si allinea sia per sentimento positivo (maggioranza: 80%) che per conciseness (minoranza: 20%), RLHF standard ha completamente ignorato la preferenza di conciseness del gruppo minoritario. Con rapporti sbilanciati (10:1 maggioranza:minoranza), RLHF con singolo reward ha raggiunto solo 42% di accuratezza sulle preferenze minoritarie vs 71.6% su quelle maggioritarie.

Il paper dimostra matematicamente che man mano che la diversità aumenta, il modello sempre più **"subdues the preferences or opinions of minority groups, leading to societal biases."** La formalizzazione matematica mostra che questo non è un artefatto di addestramento ma una limitazione strutturale fondamentale dell'aggregazione di preferenze diverse in un singolo segnale di reward.

---

### Critiche su Opacity

#### 6. Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback

**Citazione completa:**
Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback." *arXiv preprint arXiv:2307.15217*, 2023.

**Perché è rilevante:** Questo è un survey completo di oltre 250 paper su RLHF che identifica problemi fondamentali e limitazioni sistematiche. È altamente citato e rappresenta una sintesi autorevole delle critiche a RLHF.

**Come supporta il punto critico su Opacity:**

Il paper identifica tre problemi critici di opacità:

1. **Problema di misspecificazione fondamentale:** I reward model soffrono di problemi "doubly-misspecified" dove il feedback umano dipende da fattori contestuali che non possono essere contabilizzati negli esempi usati per addestrare il reward model.
2. **Reward misgeneralization:** Anche con dati di addestramento correttamente etichettati, i reward model possono **"compute reward using spurious features that are irrelevant to human preferences."**
3. **Natura black-box e sfide di valutazione:** Il paper affronta esplicitamente la difficoltà di valutare i reward model: **"When the true reward function is known, several methods can be used to judge the quality of the learned reward model. However, in most cases, reward modeling is used only when the true reward function is not known, making direct evaluation impossible."**

**Estratti chiave:**

- *"Reward models can compute reward using unexpected, possibly contingent features of the environment and are prone to causal confusion and poor out-of-distribution generalization."*
- **Reward Hacking:** *"Skalse et al. (2022) show that unhackable proxies are very rare in complex environments, and Zhuang and Hadfield-Menell (2020) prove under mild conditions that reward hacking should be expected by default."*
- **Incapacità di audit:** Il paper sottolinea che **"the output being a scalar reward is crucial"** ma questo scalare non fornisce insight sul *perché* il reward è stato assegnato, rendendo l'auditing impossibile senza strumenti aggiuntivi.

Il paper propone standard specifici di trasparenza e auditing nella Sezione 5, sottolineando che le aziende dovrebbero divulgare la loss function usata per fittare il reward model e report sulla valutazione del reward model per suggerire possibili problemi da reward model disallineato.

---

#### 7. Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts

**Citazione completa:**
Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts." In *Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2024)*, arXiv:2406.12845, 2024.

**Perché è rilevante:** Questo paper critica esplicitamente la natura black-box dei reward model standard e propone soluzioni per renderli interpretabili.

**Come supporta il punto critico su Opacity:**

**Estratti chiave:**

- *"Common RMs, such as the most popular Bradley-Terry RMs, are typically black-box models that output scores or preferences without providing human-interpretable explanations."*
- **Opacità abilita reward hacking:** *"When applying RLHF for LLM alignment, the phenomenon of reward hacking is widely observed, where the aligned LLMs generate high-reward responses (rated by the RM) that do not align with actual human preferences."*

Il paper fornisce un esempio concreto di verbosity bias: un reward model opaco che assegna 60% del peso alla lunghezza piuttosto che alla qualità, ma questo bias non può essere rilevato o corretto senza strumenti di interpretabilità.

**Necessità di verifica umana:** Gli autori sottolineano: *"Enhancing the interpretability of RMs also allows humans to verify whether RMs have similar internal decision processes to humans when acting as proxies for human preferences. We believe that this thorough human verification process could ensure that RMs are deeply and comprehensively consistent with human values."*

Il paper propone ArmoRM con MoE che decompone il reward in 19 obiettivi interpretabili (helpfulness, correctness, coherence, safety, ecc.), rendendo possibile auditare quali obiettivi guidano specifiche assegnazioni di reward.

---

#### 8. InfoRM: Mitigating Reward Hacking in RLHF via Information-Theoretic Reward Modeling

**Citazione completa:**
Yuchun Miao, Sen Zhang, Liang Ding, Rong Bao, Lefei Zhang, Dacheng Tao. "InfoRM: Mitigating Reward Hacking in RLHF via Information-Theoretic Reward Modeling." In *Advances in Neural Information Processing Systems (NeurIPS 2024)*, arXiv:2402.09345, 2024.

**Perché è rilevante:** Paper peer-reviewed da NeurIPS 2024 che affronta specificamente come l'opacità dei reward model porta a reward hacking.

**Come supporta il punto critico su Opacity:**

Il paper identifica che **"reward hacking...primarily arises from reward misgeneralization, where reward models (RMs) compute reward using spurious features that are irrelevant to human preferences."** Questo crea opacità perché la base decisionale effettiva del reward model è nascosta e differisce dalle preferenze intese.

**Estratti chiave:**

- **Rappresentazioni latenti black-box:** Il paper dimostra che reward model standard operano come black box dove le rappresentazioni interne non sono accessibili o interpretabili.
- **Misallineamento nascosto:** *"We further identify a correlation between overoptimization and outliers in the IB latent space of InfoRM, establishing it as a promising tool for detecting reward overoptimization."*
- **Difficoltà nel rilevare problemi:** Reward model standard rendono difficile rilevare quando le cose vanno male. Il paper propone il "Cluster Separation Index (CSI)" come strumento per quantificare deviazioni—questa metrica è necessaria precisamente perché il decision-making del reward model è opaco.

Il paper dimostra sperimentalmente attraverso multiple scale di modelli (70M a 7B parametri) che i reward model sviluppano consistentemente bias nascosti e misallineamenti che non sono visibili senza strumenti di analisi specializzati. Gli autori enfatizzano che il loro approccio fornisce strumenti per guardare dentro la black box dei reward model e rilevare quando stanno prendendo decisioni basate su feature sbagliate, abilitando migliore auditing e supervisione.

---

## Sintesi dei temi trasversali

Le pubblicazioni identificate convergono su tre critiche fondamentali a RLHF:

**1. Annotator Bias e Task Simplification:**

- Tassi di disaccordo elevati (23-37%) anche tra annotatori addestrati
- Gli annotatori hanno difficoltà con risposte complesse e sfumate
- Comportamento di pattern-following porta a dati di preferenza semplificati e omogenei
- RLHF amplifica questi bias, assegnando >99% di probabilità alle opinioni dominanti

**2. Collapse of Pluralism:**

- Risultati di impossibilità formali dimostrano che singoli reward model non possono rappresentare preferenze diverse
- Le preferenze minoritarie vengono sistematicamente eliminate (accuratezza 42% vs 71.6%)
- RLHF riduce tutti i tre tipi di pluralismo (Overton, Steerable, Distributional)
- L'aggregazione di preferenze attraverso un singolo reward elimina matematicamente viewpoint minoritari

**3. Opacity:**

- Reward model producono reward scalari senza spiegazioni, rendendo opaco il loro decision-making
- Reward model apprendono feature spurie irrilevanti alle preferenze umane
- Impossibile valutare direttamente i reward model senza ground truth
- Reward hacking è abilitato e nascosto dall'opacità dei reward model

Queste pubblicazioni, tutte peer-reviewed da venue di alto impatto (NeurIPS, ICML, EMNLP, CVPR, EACL, JASA), forniscono una base solida per criticare i limiti di RLHF nella Sezione 1.2 del paper su RLCF.
