# MERL-T Environment Configuration Template
# Copy this file to .env and fill in your values
# DO NOT commit .env to version control!

# =============================================================================
# DATABASE
# =============================================================================

# SQLite (Development - Default)
DATABASE_URL=sqlite+aiosqlite:///./rlcf.db

# PostgreSQL (Production - Phase 3+)
# DATABASE_URL=postgresql+asyncpg://user:password@localhost:5432/merl_t_db

# =============================================================================
# API KEYS
# =============================================================================

# OpenRouter API Key (for AI response generation)
# Get your key at: https://openrouter.ai/
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Admin API Key (for protected admin endpoints)
# Generate a secure random key: python -c "import secrets; print(secrets.token_urlsafe(32))"
ADMIN_API_KEY=your_admin_api_key_here

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================

# Backend Host & Port
HOST=0.0.0.0
PORT=8000

# Enable auto-reload for development
RELOAD=true

# =============================================================================
# CORS CONFIGURATION
# =============================================================================

# Allowed origins for CORS (comma-separated)
CORS_ORIGINS=http://localhost:3000,http://localhost:5173,http://127.0.0.1:3000

# =============================================================================
# FRONTEND CONFIGURATION
# =============================================================================

# Backend API URL (used by frontend)
VITE_API_BASE_URL=http://localhost:8000

# =============================================================================
# AI MODEL CONFIGURATION
# =============================================================================

# Default AI Model (from OpenRouter)
# Options: openai/gpt-4o, openai/gpt-3.5-turbo, anthropic/claude-3-sonnet, meta-llama/llama-3-70b
AI_MODEL=openai/gpt-3.5-turbo

# Model Temperature (0.0 - 1.0)
AI_TEMPERATURE=0.7

# Max Tokens
AI_MAX_TOKENS=1000

# =============================================================================
# RLCF CONFIGURATION
# =============================================================================

# Authority Weights (sum must equal 1.0)
AUTHORITY_WEIGHT_BASELINE=0.4
AUTHORITY_WEIGHT_TRACK_RECORD=0.4
AUTHORITY_WEIGHT_RECENT_PERFORMANCE=0.2

# Disagreement Threshold (0.0 - 1.0)
DISAGREEMENT_THRESHOLD=0.3

# Track Record Update Factor (Î» in exponential smoothing)
TRACK_RECORD_LAMBDA=0.95

# =============================================================================
# KNOWLEDGE GRAPH & CACHING
# =============================================================================

# Neo4j
# Uncomment these lines when using Phase 2 features
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=merl_t_password
NEO4J_DATABASE=neo4j

# Redis
# Uncomment when using Phase 2 features
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=
# REDIS_PASSWORD=your_redis_password_here  # Set if Redis has authentication

# =============================================================================
# LLM Router + Retrieval Agents CONFIGURATION
# =============================================================================

# LLM Router Model (OpenRouter)
ROUTER_MODEL=anthropic/claude-3.5-sonnet
ROUTER_TEMPERATURE=0.1

# Reasoning Experts Model (OpenRouter)
EXPERT_MODEL=anthropic/claude-3.5-sonnet

# Synthesizer Model (OpenRouter)
SYNTHESIZER_MODEL=anthropic/claude-3.5-sonnet

# Iteration Settings
MAX_ITERATIONS=3

# Custom Norma Controller API (Visualex microservice)
# For Docker deployment: http://visualex:5000 (service name)
# For native development: http://localhost:5000 (local port)
NORMA_API_URL=http://localhost:5000

# Sentenze API (placeholder/mock - configure when available)
# For Docker deployment: http://sentenze:5001 (service name)
# For native development: http://localhost:5001 (local port)
SENTENZE_API_URL=http://localhost:5001

# =============================================================================
# Vector Database + Embeddings CONFIGURATION
# =============================================================================

# Qdrant (Vector Database)
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_GRPC_PORT=6334
QDRANT_COLLECTION_NAME=legal_corpus
# QDRANT_API_KEY=  # Optional, leave empty for local development

# E5-large Multilingual Embeddings
EMBEDDING_MODEL=sentence-transformers/multilingual-e5-large
EMBEDDING_DEVICE=cpu  # Options: cpu, cuda (use cuda if GPU available)
EMBEDDING_BATCH_SIZE=32
EMBEDDING_NORMALIZE=true
EMBEDDING_DIMENSION=1024  # E5-large output dimension

# =============================================================================
# Orchestration Database + Caching CONFIGURATION
# =============================================================================

# Orchestration API Database (separate from RLCF database)
# This database stores query tracking, results, and feedback
# SQLite (Development - Default)
ORCHESTRATION_DATABASE_URL=sqlite+aiosqlite:///./orchestration.db

# PostgreSQL (Production - Recommended for Week 7+)
# For Docker deployment:
# ORCHESTRATION_DATABASE_URL=postgresql+asyncpg://merl_t:merl_t_password@postgres-orchestration:5432/orchestration_db
# For native development (use port 5433 to avoid conflicts):
# ORCHESTRATION_DATABASE_URL=postgresql+asyncpg://merl_t:merl_t_password@localhost:5433/orchestration_db

# Redis Caching (optional - can be disabled for development)
# Set to false to disable Redis caching and rate limiting
REDIS_ENABLED=true

# =============================================================================
# WEEK 8 CONFIGURATION (API Authentication & Rate Limiting)
# =============================================================================

# API Authentication (Week 8 Day 4)
# WARNING: These are development-only keys. CHANGE IN PRODUCTION!

# Admin API Key (unlimited tier, full access)
# Development key: "merl-t-admin-key-dev-only-change-in-production"
# SHA-256 hash: 8d969eef6ecad3c29a3a629280e686cf0c3f5d5a86aff3ca12020c923adc6c92
API_KEY_ADMIN=merl-t-admin-key-dev-only-change-in-production

# User API Key (standard tier, 100 req/hour)
# Development key: "merl-t-user-key-dev-only"
# SHA-256 hash: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
API_KEY_USER=merl-t-user-key-dev-only

# Rate Limiting Configuration
# Enable/disable rate limiting (true/false)
RATE_LIMITING_ENABLED=true

# Rate limit tiers (requests per hour)
RATE_LIMIT_UNLIMITED=999999  # Admin tier
RATE_LIMIT_PREMIUM=1000      # Premium tier
RATE_LIMIT_STANDARD=100      # Standard tier (default)
RATE_LIMIT_LIMITED=10        # Limited/Guest tier

# Generate new API keys with:
# python -c "import secrets; print(secrets.token_urlsafe(32))"
# Then hash with:
# python -c "import hashlib; key='your-key-here'; print(hashlib.sha256(key.encode()).hexdigest())"

# For quick development, use the default key (already in database):
# X-API-KEY: supersecretkey
# WARNING: This key is FOR DEVELOPMENT ONLY! Change in production!

# =============================================================================
# WEEK 9+ CONFIGURATION (KG Ingestion API)
# =============================================================================

# Ingestion API Configuration (Port 8002)
# Manages batch knowledge graph ingestion from multiple sources

# Ingestion Database (can share with orchestration or use separate)
INGESTION_DATABASE_URL=sqlite+aiosqlite:///./ingestion.db
# For production PostgreSQL:
# INGESTION_DATABASE_URL=postgresql+asyncpg://merl_t:merl_t_password@localhost:5432/ingestion_db

# Ingestion API Host & Port
INGESTION_HOST=0.0.0.0
INGESTION_PORT=8002

# Batch Processing Configuration
INGESTION_MAX_BATCH_SIZE=1000  # Maximum entities per batch
INGESTION_TIMEOUT_SECONDS=300  # Batch processing timeout (5 minutes)
INGESTION_WORKER_THREADS=4     # Concurrent workers for batch processing

# External Data Sources (for KG enrichment)
# All 5 sources are configured in backend/preprocessing/kg_enrichment_service.py:
# 1. Normattiva (official legislative database)
# 2. Brocardi (legal definitions and jurisprudence)
# 3. WikiData (structured knowledge base)
# 4. EUR-Lex (EU legislation)
# 5. Community Sources (user contributions)

# Normattiva API Configuration (already set above in NORMA_API_URL)
# Uses visualex microservice on port 5000

# Enable/disable specific enrichment sources
ENABLE_NORMATTIVA=true
ENABLE_BROCARDI=true
ENABLE_WIKIDATA=true
ENABLE_EURLEX=true
ENABLE_COMMUNITY_SOURCES=true

# =============================================================================
# SYSTEM ARCHITECTURE (5 Services)
# =============================================================================

# Port Mapping:
# 5000  - visualex API (Quart) - Normattiva search & norm retrieval
# 8000  - Orchestration API (FastAPI) - Query execution, LLM Router, Experts
# 8001  - RLCF API (FastAPI) - Task management, authority scoring, feedback
# 8002  - Ingestion API (FastAPI) - KG batch ingestion, entity validation
# 3000  - Frontend (React/Vite) - User interface

# Database Architecture:
# - rlcf.db: RLCF Framework data (tasks, users, responses, feedback)
# - orchestration.db: Query tracking, results, user feedback, API keys
# - ingestion.db: Batch processing metadata, entity staging (optional - can share with orchestration.db)
# - Neo4j/Memgraph: Knowledge Graph storage (production)
# - Qdrant: Vector embeddings for semantic search
# - Redis: Caching and rate limiting

# =============================================================================
# FUTURE PHASE 3-6 CONFIGURATION (commented out)
# =============================================================================

# OpenAI Embeddings (Phase 3)
# OPENAI_API_KEY=your_openai_api_key_here
# EMBEDDING_MODEL=text-embedding-3-large

# Voyage AI Embeddings (Phase 3 - Alternative)
# VOYAGE_API_KEY=your_voyage_api_key_here
# VOYAGE_MODEL=voyage-multilingual-2

# Observability (Phase 6 - SigNoz)
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# =============================================================================
# LOGGING
# =============================================================================

# Log Level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Log Format (json, text)
LOG_FORMAT=text

# =============================================================================
# TESTING
# =============================================================================

# Test Database (separate from development)
TEST_DATABASE_URL=sqlite+aiosqlite:///./test_rlcf.db

# =============================================================================
# NOTES
# =============================================================================

# 1. Copy this file to .env: cp .env.template .env
# 2. Fill in your actual values (especially API keys)
# 3. Add .env to .gitignore (already done)
# 4. For production, use secure environment variable injection (e.g., Kubernetes Secrets)
