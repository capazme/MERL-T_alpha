# MERL-T Environment Configuration Template
# Copy this file to .env and fill in your values
# DO NOT commit .env to version control!

# =============================================================================
# DATABASE
# =============================================================================

# SQLite (Development - Default)
DATABASE_URL=sqlite+aiosqlite:///./rlcf.db

# PostgreSQL (Production - Phase 3+)
# DATABASE_URL=postgresql+asyncpg://user:password@localhost:5432/merl_t_db

# =============================================================================
# API KEYS
# =============================================================================

# OpenRouter API Key (for AI response generation)
# Get your key at: https://openrouter.ai/
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Admin API Key (for protected admin endpoints)
# Generate a secure random key: python -c "import secrets; print(secrets.token_urlsafe(32))"
ADMIN_API_KEY=your_admin_api_key_here

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================

# Backend Host & Port
HOST=0.0.0.0
PORT=8000

# Enable auto-reload for development
RELOAD=true

# =============================================================================
# CORS CONFIGURATION
# =============================================================================

# Allowed origins for CORS (comma-separated)
CORS_ORIGINS=http://localhost:3000,http://localhost:5173,http://127.0.0.1:3000

# =============================================================================
# FRONTEND CONFIGURATION
# =============================================================================

# Backend API URL (used by frontend)
VITE_API_BASE_URL=http://localhost:8000

# =============================================================================
# AI MODEL CONFIGURATION
# =============================================================================

# Default AI Model (from OpenRouter)
# Options: openai/gpt-4o, openai/gpt-3.5-turbo, anthropic/claude-3-sonnet, meta-llama/llama-3-70b
AI_MODEL=openai/gpt-3.5-turbo

# Model Temperature (0.0 - 1.0)
AI_TEMPERATURE=0.7

# Max Tokens
AI_MAX_TOKENS=1000

# =============================================================================
# RLCF CONFIGURATION
# =============================================================================

# Authority Weights (sum must equal 1.0)
AUTHORITY_WEIGHT_BASELINE=0.4
AUTHORITY_WEIGHT_TRACK_RECORD=0.4
AUTHORITY_WEIGHT_RECENT_PERFORMANCE=0.2

# Disagreement Threshold (0.0 - 1.0)
DISAGREEMENT_THRESHOLD=0.3

# Track Record Update Factor (Î» in exponential smoothing)
TRACK_RECORD_LAMBDA=0.95

# =============================================================================
# PHASE 2 CONFIGURATION (Knowledge Graph + Caching)
# =============================================================================

# Neo4j (Phase 2 - Knowledge Graph)
# Uncomment these lines when using Phase 2 features
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=merl_t_password
NEO4J_DATABASE=neo4j

# Redis (Phase 2 - Caching)
# Uncomment when using Phase 2 features
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=
# REDIS_PASSWORD=your_redis_password_here  # Set if Redis has authentication

# =============================================================================
# WEEK 6 CONFIGURATION (LLM Router + Retrieval Agents)
# =============================================================================

# LLM Router Model (OpenRouter)
ROUTER_MODEL=anthropic/claude-3.5-sonnet
ROUTER_TEMPERATURE=0.1

# Reasoning Experts Model (OpenRouter)
EXPERT_MODEL=anthropic/claude-3.5-sonnet

# Synthesizer Model (OpenRouter)
SYNTHESIZER_MODEL=anthropic/claude-3.5-sonnet

# Iteration Settings
MAX_ITERATIONS=3

# Custom Norma Controller API (Visualex microservice)
# For Docker deployment: http://visualex:5000 (service name)
# For native development: http://localhost:5000 (local port)
NORMA_API_URL=http://localhost:5000

# Sentenze API (placeholder/mock - configure when available)
# For Docker deployment: http://sentenze:5001 (service name)
# For native development: http://localhost:5001 (local port)
SENTENZE_API_URL=http://localhost:5001

# =============================================================================
# PHASE 3 CONFIGURATION (Week 6 Day 2 - Vector Database + Embeddings)
# =============================================================================

# Qdrant (Vector Database)
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_GRPC_PORT=6334
QDRANT_COLLECTION_NAME=legal_corpus
# QDRANT_API_KEY=  # Optional, leave empty for local development

# E5-large Multilingual Embeddings
EMBEDDING_MODEL=sentence-transformers/multilingual-e5-large
EMBEDDING_DEVICE=cpu  # Options: cpu, cuda (use cuda if GPU available)
EMBEDDING_BATCH_SIZE=32
EMBEDDING_NORMALIZE=true
EMBEDDING_DIMENSION=1024  # E5-large output dimension

# =============================================================================
# FUTURE PHASE 3-6 CONFIGURATION (commented out)
# =============================================================================

# OpenAI Embeddings (Phase 3)
# OPENAI_API_KEY=your_openai_api_key_here
# EMBEDDING_MODEL=text-embedding-3-large

# Voyage AI Embeddings (Phase 3 - Alternative)
# VOYAGE_API_KEY=your_voyage_api_key_here
# VOYAGE_MODEL=voyage-multilingual-2

# Observability (Phase 6 - SigNoz)
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# =============================================================================
# LOGGING
# =============================================================================

# Log Level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Log Format (json, text)
LOG_FORMAT=text

# =============================================================================
# TESTING
# =============================================================================

# Test Database (separate from development)
TEST_DATABASE_URL=sqlite+aiosqlite:///./test_rlcf.db

# =============================================================================
# NOTES
# =============================================================================

# 1. Copy this file to .env: cp .env.template .env
# 2. Fill in your actual values (especially API keys)
# 3. Add .env to .gitignore (already done)
# 4. For production, use secure environment variable injection (e.g., Kubernetes Secrets)
